Welcome to CacheFlow!
=====================

**CacheFlow** is a fast and easy-to-use library for LLM inference and serving.
Its core features include:

- Seamless integration with popular HuggingFace models
- Efficient block-based management for KV cache
- Advanced batching mechanism
- Optimized CUDA kernels
- Tensor parallelism support for multi-GPU inference
- Efficient support for various decoding algorithms such as parallel sampling and beam search
- OpenAI-compatible API

For more information, please refer to:

* Blog post: ``
* Slides: ``
* Paper: `CacheFlow: Efficient Memory Management for Large Language Model Serving <https:>`_


Documentation
-------------

.. toctree::
   :maxdepth: 1
   :caption: Getting Started

   getting_started/installation
   getting_started/quickstart

.. toctree::
   :maxdepth: 1
   :caption: Models

   models/supported_models
   models/adding_model
