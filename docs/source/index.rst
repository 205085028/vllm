Welcome to CacheFlow!
=====================

**CacheFlow** is a fast and easy-to-use library for LLM inference and serving.
Its core features include:

- Seamless integration with popular HuggingFace models
- Efficient management for cached attention keys and values
- Advanced batching mechanism
- Optimized CUDA kernels
- Efficient support for various decoding algorithms such as parallel sampling and beam search
- Tensor parallelism support for multi-GPU inference
- OpenAI-compatible API

For more information, please refer to our [blog post]().


Documentation
-------------

.. toctree::
   :maxdepth: 1
   :caption: Getting Started

   getting_started/installation
   getting_started/quickstart

.. toctree::
   :maxdepth: 1
   :caption: Models

   models/supported_models
   models/adding_model
