{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccee9b39594e42cba27071677f2574db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "lora_conf = snapshot_download(repo_id=\"tybritten/lora-for-starcoder\")\n",
    "\n",
    "lora_request=LoRARequest(\"testing adapter\", 1, lora_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 12:37:43 config.py:413] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 12:37:43,971\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.0.15.216:6379...\n",
      "2024-03-12 12:37:43,979\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-eu6m2d8n6xkuxbxenpykevvc1t.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-03-12 12:37:43,993\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_9a79c65897c11fe3d6091b93537505ad.zip' (2.10MiB) to Ray cluster...\n",
      "2024-03-12 12:37:44,002\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_9a79c65897c11fe3d6091b93537505ad.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 12:37:45 llm_engine.py:79] Initializing an LLM engine with config: model='bigcode/starcoder', tokenizer='bigcode/starcoder', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-12 12:37:59 weight_utils.py:163] Using model weights format ['*.bin']\n",
      "\u001b[36m(RayWorkerVllm pid=67380)\u001b[0m INFO 03-12 12:38:00 weight_utils.py:163] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigcode/starcoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/entrypoints/llm.py:109\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     91\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     92\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     93\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    108\u001b[0m )\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:371\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    369\u001b[0m placement_group \u001b[38;5;241m=\u001b[39m initialize_cluster(parallel_config)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m             \u001b[49m\u001b[43mplacement_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:123\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, placement_group, log_stats)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_workers()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Create the scheduler.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m Scheduler(scheduler_config, cache_config, lora_config)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:323\u001b[0m, in \u001b[0;36mLLMEngine._init_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m\"\"\"Profiles the memory usage and initializes the KV cache.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03mThe engine will first conduct a profiling of the existing memory usage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    by adjusting the `gpu_memory_utilization` parameters.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Get the maximum number of blocks that can be allocated on GPU and CPU.\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m num_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprofile_num_available_blocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu_swap_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswap_space_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Since we use a shared centralized controller, we take the minimum\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# number of blocks across all workers to make sure all the memory\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# operators can be applied to all workers.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m num_gpu_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m num_blocks)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:1009\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, use_ray_compiled_dag, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/worker/worker.py:126\u001b[0m, in \u001b[0;36mWorker.profile_num_available_blocks\u001b[0;34m(self, block_size, gpu_memory_utilization, cpu_swap_space, cache_dtype)\u001b[0m\n\u001b[1;32m    122\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Calculate the number of blocks that can be allocated with the\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# profiled peak memory.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/worker/model_runner.py:600\u001b[0m, in \u001b[0;36mModelRunner.profile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m     lora_id \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    595\u001b[0m     dummy_lora_request \u001b[38;5;241m=\u001b[39m LoRARequest(\n\u001b[1;32m    596\u001b[0m         lora_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    597\u001b[0m         lora_int_id\u001b[38;5;241m=\u001b[39mlora_id,\n\u001b[1;32m    598\u001b[0m         lora_local_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/not/a/real/path\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    599\u001b[0m     )\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_dummy_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_lora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLORA_WARMUP_RANK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     dummy_lora_requests\u001b[38;5;241m.\u001b[39mappend(dummy_lora_request)\n\u001b[1;32m    603\u001b[0m dummy_lora_requests_per_seq \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    604\u001b[0m     dummy_lora_requests[idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(dummy_lora_requests)]\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_num_seqs)\n\u001b[1;32m    606\u001b[0m ]\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/lora/worker_manager.py:167\u001b[0m, in \u001b[0;36mWorkerLoRAManager.add_dummy_lora\u001b[0;34m(self, lora_request, rank)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lora_request\u001b[38;5;241m.\u001b[39mlora_int_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_loras():\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_manager\u001b[38;5;241m.\u001b[39madd_lora(\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lora_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dummy_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_int_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_modules\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/lora/models.py:474\u001b[0m, in \u001b[0;36mLoRAModelManager.create_dummy_lora\u001b[0;34m(self, lora_id, rank, embedding_modules)\u001b[0m\n\u001b[1;32m    463\u001b[0m         lora \u001b[38;5;241m=\u001b[39m LoRALayerWeights\u001b[38;5;241m.\u001b[39mcreate_dummy_lora_weights(\n\u001b[1;32m    464\u001b[0m             module_name,\n\u001b[1;32m    465\u001b[0m             input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    470\u001b[0m             embeddings_tensor_dim\u001b[38;5;241m=\u001b[39membeddings_tensor_dim)\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m         lora \u001b[38;5;241m=\u001b[39m LoRALayerWeights\u001b[38;5;241m.\u001b[39mcreate_dummy_lora_weights(\n\u001b[1;32m    473\u001b[0m             module_name,\n\u001b[0;32m--> 474\u001b[0m             \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_a_stacked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    475\u001b[0m             module\u001b[38;5;241m.\u001b[39mlora_b_stacked\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    476\u001b[0m             rank,\n\u001b[1;32m    477\u001b[0m             module\u001b[38;5;241m.\u001b[39mlora_a_stacked\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    478\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    479\u001b[0m         )\n\u001b[1;32m    480\u001b[0m     lora\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 12:39:03,152\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RayWorkerVllm.execute_method()\u001b[39m (pid=67380, ip=10.0.15.216, actor_id=4ef9adacd7f341453e1a36350c000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f24b1fc02b0>)\n",
      "  File \"/tmp/ray/session_2024-03-12_10-00-39_161158_5105/runtime_resources/working_dir_files/_ray_pkg_9a79c65897c11fe3d6091b93537505ad/vllm/engine/ray_utils.py\", line 37, in execute_method\n",
      "    return executor(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-12_10-00-39_161158_5105/runtime_resources/working_dir_files/_ray_pkg_9a79c65897c11fe3d6091b93537505ad/vllm/worker/worker.py\", line 126, in profile_num_available_blocks\n",
      "    self.model_runner.profile_run()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-12_10-00-39_161158_5105/runtime_resources/working_dir_files/_ray_pkg_9a79c65897c11fe3d6091b93537505ad/vllm/worker/model_runner.py\", line 600, in profile_run\n",
      "    self.lora_manager.add_dummy_lora(dummy_lora_request,\n",
      "  File \"/tmp/ray/session_2024-03-12_10-00-39_161158_5105/runtime_resources/working_dir_files/_ray_pkg_9a79c65897c11fe3d6091b93537505ad/vllm/lora/worker_manager.py\", line 167, in add_dummy_lora\n",
      "    self._lora_manager.create_dummy_lora(lora_request.lora_int_id,\n",
      "  File \"/tmp/ray/session_2024-03-12_10-00-39_161158_5105/runtime_resources/working_dir_files/_ray_pkg_9a79c65897c11fe3d6091b93537505ad/vllm/lora/models.py\", line 474, in create_dummy_lora\n",
      "    module.lora_a_stacked.shape[-1],\n",
      "AttributeError: 'tuple' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +6m47s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"bigcode/starcoder\", tensor_parallel_size=2, enable_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['base_model.model.transformer.h.0.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.6.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.6.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.6.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.6.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.7.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.7.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.7.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.7.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.8.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.8.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.8.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.8.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.9.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.9.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.9.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.9.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.10.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.10.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.10.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.10.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.11.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.11.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.11.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.11.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.12.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.12.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.12.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.12.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.12.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.12.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.13.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.13.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.13.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.13.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.13.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.13.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.14.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.14.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.14.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.14.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.14.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.14.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.15.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.15.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.15.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.15.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.15.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.15.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.16.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.16.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.16.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.16.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.16.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.16.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.17.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.17.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.17.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.17.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.17.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.17.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.18.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.18.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.18.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.18.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.18.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.18.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.19.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.19.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.19.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.19.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.19.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.19.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.20.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.20.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.20.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.20.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.20.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.20.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.21.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.21.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.21.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.21.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.21.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.21.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.22.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.22.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.22.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.22.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.22.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.22.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.23.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.23.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.23.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.23.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.23.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.23.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.24.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.24.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.24.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.24.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.24.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.24.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.25.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.25.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.25.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.25.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.25.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.25.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.26.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.26.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.26.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.26.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.26.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.26.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.27.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.27.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.27.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.27.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.27.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.27.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.28.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.28.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.28.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.28.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.28.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.28.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.29.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.29.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.29.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.29.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.29.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.29.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.30.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.30.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.30.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.30.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.30.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.30.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.31.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.31.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.31.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.31.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.31.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.31.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.32.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.32.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.32.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.32.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.32.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.32.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.33.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.33.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.33.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.33.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.33.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.33.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.34.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.34.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.34.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.34.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.34.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.34.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.35.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.35.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.35.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.35.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.35.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.35.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.36.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.36.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.36.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.36.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.36.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.36.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.37.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.37.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.37.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.37.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.37.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.37.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.38.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.38.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.38.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.38.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.38.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.38.mlp.c_proj.lora_B.weight', 'base_model.model.transformer.h.39.attn.c_attn.lora_A.weight', 'base_model.model.transformer.h.39.attn.c_attn.lora_B.weight', 'base_model.model.transformer.h.39.attn.c_proj.lora_A.weight', 'base_model.model.transformer.h.39.attn.c_proj.lora_B.weight', 'base_model.model.transformer.h.39.mlp.c_proj.lora_A.weight', 'base_model.model.transformer.h.39.mlp.c_proj.lora_B.weight'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "lora_state_dict = torch.load(lora_request.lora_local_path+'/adapter_model.bin')\n",
    "\n",
    "print(lora_state_dict.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
