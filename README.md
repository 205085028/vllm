# vLLM

vLLM is a fast and easy-to-use library for LLM inference and serving.

## Latest News

- [2023/06] We officially released vLLM! vLLM has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid April. Check out our [blog post]().

## Getting Started

Visit our [documentation](https://llm-serving-cacheflow.readthedocs-hosted.com/_/sharing/Cyo52MQgyoAWRQ79XA4iA2k8euwzzmjY?next=/en/latest/) to get started.
- [Installation](https://llm-serving-cacheflow.readthedocs-hosted.com/_/sharing/Cyo52MQgyoAWRQ79XA4iA2k8euwzzmjY?next=/en/latest/getting_started/installation.html)
- [Quickstart](https://llm-serving-cacheflow.readthedocs-hosted.com/_/sharing/Cyo52MQgyoAWRQ79XA4iA2k8euwzzmjY?next=/en/latest/getting_started/quickstart.html)
- [Supported Models](https://llm-serving-cacheflow.readthedocs-hosted.com/_/sharing/Cyo52MQgyoAWRQ79XA4iA2k8euwzzmjY?next=/en/latest/models/supported_models.html)

## Key Features

vLLM comes with many powerful features that include:

- Seamless integration with popular HuggingFace models
- Efficient management for cached attention keys and values
- Advanced batching mechanism
- Optimized CUDA kernels
- Efficient support for various decoding algorithms such as parallel sampling and beam search
- Tensor parallelism support for multi-GPU inference
- OpenAI-compatible API

## Performance


## Contributing

As an open-source project in a fast-evolving field, we welcome any contributions and collaborations.
For guidance on how to contribute, please check out [CONTRIBUTING.md](./CONTRIBUTING.md).
