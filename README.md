# FluentFlow

FluentFlow is a fast and easy-to-use library for LLM inference and serving.
Using efficient memory management techniques, FluentFlow delivers x-x higher throughput than state-of-the-art systems.
FluentFlow has powered [LMSys Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid April, significantly reducing its operational costs.

## News

- [2023/06] FluentFlow was officially released! Please check out our [blog post]() and [paper]().

## Getting Started

Visit our [documentation]() to get started.
- [Installation]()
- [Quickstart]()
- [OpenAI-compatible API]()
- [Supported Models]()

## Key Features

FluentFlow comes with many powerful features that include:

- Efficient block-based management for KV cache
- Advanced batching mechanism
- Optimized CUDA kernels
- Tensor parallelism support for multi-GPU inference
- OpenAI-compatible API

## Performance


## Contributing

As an open-source project in a fast-evolving field, we welcome any contributions and collaborations.
For guidance on how to contribute, please check out [CONTRIBUTING.md]().
