# CacheFlow

CacheFlow is a fast and easy-to-use library for LLM inference and serving.

## Latest News

- [2023/06] We officially release CacheFlow, which has powered [LMSys Vicuna and Chatbot Arena]() since mid April! Please check out our [blog post](), [slides](), and [paper]().

## Getting Started

Visit our [documentation]() to get started.
- [Installation]()
- [Quickstart]()
- [Supported Models]()

## Key Features

CacheFlow comes with many powerful features that include:

- Seamless integration with popular HuggingFace models
- Efficient block-based management for KV cache
- Advanced batching mechanism
- Optimized CUDA kernels
- Tensor parallelism support for multi-GPU inference
- Efficient support for various decoding algorithms such as parallel sampling and beam search.
- OpenAI-compatible API

## Performance


## Contributing

As an open-source project in a fast-evolving field, we welcome any contributions and collaborations.
For guidance on how to contribute, please check out [CONTRIBUTING.md](./CONTRIBUTING.md).
