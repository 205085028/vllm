{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (319805099.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [10], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    return ret\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class Loading_Optimizer:\n",
    "    def __init__(self, per_chunk_info: List[Dict[str, int]], gpu_waiting_time) -> None:\n",
    "        self.num_chunks = len(per_chunk_info)\n",
    "        self.per_chunk_info = per_chunk_info\n",
    "        self.gpu_waiting_time = gpu_waiting_time\n",
    "    \n",
    "    #for each token chunk, output number of proportion needed and compute the rest.\n",
    "    #we could also do a loading graph\n",
    "    def get_loading_plan(self) -> List[Tuple[int, int, int]]: \n",
    "        ret = []\n",
    "        \n",
    "        for chunk in self.per_chunk_info:\n",
    "            prefetch_proportion = 0\n",
    "\n",
    "            #Prefetch plan. TODO what if the other process is using the same bw.\n",
    "            if (chunk['tier'] > 2):\n",
    "                time_transmission = chunk['size'] / chunk['bw']\n",
    "                if (time_transmission > self.gpu_waiting_time):\n",
    "                    ret.append[1, 0, 0]\n",
    "                else:\n",
    "                    prefetch_proportion = self.gpu_waiting_time / time_transmission\n",
    "                    chunk['size'] -= prefetch_proportion * chunk['bw']\n",
    "            \n",
    "            #Now we calculate the pipeline.\n",
    "            \n",
    "            \n",
    "\n",
    "                \n",
    "        return ret\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiting_time = 0\n",
    "dict1 = {\"tier\": 3, \"bw\": 0.5, 'size': 0.1}\n",
    "dict2 = {\"tier\": 3, \"bw\": 0.5, 'size': 0.1}\n",
    "dict3 = {\"tier\": 2, \"bw\": 5, 'size': 0.1}\n",
    "dict4 = {\"tier\": 2, \"bw\": 5, 'size': 0.1} #tier x later bw is from the device to cpu\n",
    "\n",
    "opt = Loading_Optimizer([dict1, dict2, dict3, dict4], gpu_waiting_time=waiting_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ae-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
